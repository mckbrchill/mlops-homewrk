{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7706c4e-6d38-4ee0-996d-c3887f6e6468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark==2.0.1 in ./.local/lib/python3.8/site-packages (2.0.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: urllib3==1.26.6 in ./.local/lib/python3.8/site-packages (1.26.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests==2.25.1 in ./.local/lib/python3.8/site-packages (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.local/lib/python3.8/site-packages (from requests==2.25.1) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests==2.25.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests==2.25.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests==2.25.1) (2024.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-2.14.3-py3-none-any.whl (25.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.8 MB 25 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6,>=5.0.0\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting gunicorn<23; platform_system != \"Windows\"\n",
      "  Downloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 2.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle<4\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: pandas<3 in /opt/conda/lib/python3.8/site-packages (from mlflow) (1.1.3)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from mlflow) (2.0.25)\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.8/site-packages (from mlflow) (0.23.2)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Downloading sqlparse-0.5.0-py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 3.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging<25 in /opt/conda/lib/python3.8/site-packages (from mlflow) (23.2)\n",
      "Requirement already satisfied: numpy<2 in /opt/conda/lib/python3.8/site-packages (from mlflow) (1.19.2)\n",
      "Collecting alembic!=1.10.0,<2\n",
      "  Downloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 83.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opentelemetry-sdk<3,>=1.9.0\n",
      "  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 90.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.17.3 in ./.local/lib/python3.8/site-packages (from mlflow) (2.25.1)\n",
      "Collecting docker<8,>=4.0.0\n",
      "  Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 69.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Jinja2<4,>=2.11; platform_system != \"Windows\" in /opt/conda/lib/python3.8/site-packages (from mlflow) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.8/site-packages (from mlflow) (7.0.1)\n",
      "Collecting Flask<4\n",
      "  Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 16.2 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting gitpython<4,>=3.1.9\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[K     |████████████████████████████████| 207 kB 91.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click<9,>=7.0\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 9.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.8/site-packages (from mlflow) (6.0.1)\n",
      "Collecting pyarrow<16,>=4.0.0\n",
      "  Downloading pyarrow-15.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.5 MB 22 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.8/site-packages (from mlflow) (0.4)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from mlflow) (3.17.1)\n",
      "Collecting querystring-parser<2\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: pytz<2025 in /opt/conda/lib/python3.8/site-packages (from mlflow) (2023.3.post1)\n",
      "Collecting graphene<4\n",
      "  Downloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 93.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.8/site-packages (from mlflow) (3.2.2)\n",
      "Requirement already satisfied: scipy<2 in /opt/conda/lib/python3.8/site-packages (from mlflow) (1.9.3)\n",
      "Collecting markdown<4,>=3.3\n",
      "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 77.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opentelemetry-api<3,>=1.9.0\n",
      "  Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas<3->mlflow) (2.8.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))) in /opt/conda/lib/python3.8/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (4.9.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<2->mlflow) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<2->mlflow) (1.2.0)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /opt/conda/lib/python3.8/site-packages (from alembic!=1.10.0,<2->mlflow) (6.1.1)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 4.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting opentelemetry-semantic-conventions==0.46b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 79.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.local/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (1.26.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from Jinja2<4,>=2.11; platform_system != \"Windows\"->mlflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.17.0)\n",
      "Collecting Werkzeug>=3.0.0\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[K     |████████████████████████████████| 227 kB 94.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting blinker>=1.6.2\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Collecting itsdangerous>=2.1.2\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.8/site-packages (from protobuf<5,>=3.12.0->mlflow) (1.16.0)\n",
      "Collecting graphql-core<3.3,>=3.1\n",
      "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
      "\u001b[K     |████████████████████████████████| 202 kB 87.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aniso8601<10,>=8\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 2.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting graphql-relay<3.3,>=3.1\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 1.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: cachetools, gunicorn, cloudpickle, sqlparse, Mako, alembic, wrapt, deprecated, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-sdk, docker, Werkzeug, click, blinker, itsdangerous, Flask, smmap, gitdb, gitpython, pyarrow, querystring-parser, graphql-core, aniso8601, graphql-relay, graphene, markdown, mlflow\n",
      "\u001b[33m  WARNING: The script gunicorn is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script sqlformat is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script mako-render is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script alembic is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script flask is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script markdown_py is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script mlflow is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "docker 7.1.0 requires requests>=2.26.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
      "Successfully installed Flask-3.0.3 Mako-1.3.5 Werkzeug-3.0.3 alembic-1.13.2 aniso8601-9.0.1 blinker-1.8.2 cachetools-5.3.3 click-8.1.7 cloudpickle-3.0.0 deprecated-1.2.14 docker-7.1.0 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-22.0.0 itsdangerous-2.2.0 markdown-3.6 mlflow-2.14.3 opentelemetry-api-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 pyarrow-15.0.2 querystring-parser-1.2.4 smmap-5.0.1 sqlparse-0.5.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark==2.0.1\n",
    "!pip install urllib3==1.26.6\n",
    "!pip install requests==2.25.1\n",
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e4ab998-a759-4f75-88a7-e8247539fbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark==2.0.1 in ./.local/lib/python3.8/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec207ff-1775-439e-846a-ac60f88367d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "61203d8c-634d-402c-95ee-05534dcedf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)-15s %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"https://storage.yandexcloud.net\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"YCAJE4JxJM9HxqbDEUlXzJJsX\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YCNH45SiJbpy_35ywf2KolGlHKfB5lLZtbgmw2xk\"\n",
    "\n",
    "\n",
    "def get_dataframe(spark):\n",
    "    \n",
    "    s3_path = 's3a://otus-task-n3/2020-05-18.txt'\n",
    "    df = spark.read.parquet(s3_path, \n",
    "                            header=True, \n",
    "                            inferSchema=True,\n",
    "                            **{\n",
    "                                \"key\": \"\",\n",
    "                                \"secret\": \"\",\n",
    "                                \"client_kwargs\": {\"endpoint_url\": \"https://storage.yandexcloud.net\"}\n",
    "                            })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preproc(df):\n",
    "    \n",
    "    # Assuming df is DataFrame\n",
    "    df = df.withColumn('tx_datetime', F.to_timestamp(df['tx_datetime']))\n",
    "    # Time-based features\n",
    "    df = df.withColumn('is_weekend', F.when(F.dayofweek(df['tx_datetime']) >= 5, 1).otherwise(0))\n",
    "    # Sort by customer and transaction datetime\n",
    "    df = df.orderBy(['customer_id', 'tx_datetime'])\n",
    "    # Customer behavior features\n",
    "    window_spec = Window.partitionBy('customer_id').orderBy('tx_datetime')\n",
    "    # Add a lag column as a timestamp\n",
    "    df = df.withColumn('lagged_tx_datetime', F.lag('tx_datetime', 1).over(window_spec).cast('timestamp'))\n",
    "    # Calculate time_since_last_tx in seconds\n",
    "    df = df.withColumn('time_since_last_tx', \n",
    "                       (F.col('tx_datetime').cast('long') - F.col('lagged_tx_datetime').cast('long')) / F.lit(1000))\n",
    "    # Drop the intermediate lagged_tx_datetime column if not needed\n",
    "    df = df.drop('lagged_tx_datetime')\n",
    "    df = df.withColumn('avg_tx_amount_customer', F.avg('tx_amount').over(window_spec))\n",
    "    df = df.withColumn('tx_count_customer', F.count('tranaction_id').over(window_spec))\n",
    "    df = df.withColumn('var_tx_amount_customer', F.stddev('tx_amount').over(window_spec))\n",
    "    # Terminal-based features\n",
    "    window_spec_terminal = Window.partitionBy('terminal_id').orderBy('tx_datetime')\n",
    "    df = df.withColumn('avg_tx_amount_terminal', F.avg('tx_amount').over(window_spec_terminal))\n",
    "    df = df.withColumn('tx_count_terminal', F.count('tranaction_id').over(window_spec_terminal))\n",
    "    df = df.withColumn('var_tx_amount_terminal', F.stddev('tx_amount').over(window_spec_terminal))\n",
    "    \n",
    "    # Convert boolean column to binary (1/0)\n",
    "    df = df.withColumn('is_weekend', F.col('is_weekend').cast('integer'))\n",
    "    # Drop rows with null values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def scale(df):\n",
    "    \n",
    "    numeric_columns = ['tx_amount', 'time_since_last_tx', 'avg_tx_amount_customer', 'tx_count_customer',\n",
    "                       'var_tx_amount_customer', 'avg_tx_amount_terminal', 'tx_count_terminal', 'var_tx_amount_terminal']\n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"scaled_features\", outputCol=\"features\")\n",
    "    assembler = VectorAssembler(inputCols=numeric_columns, outputCol=\"scaled_features\")\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    df = pipeline_model.transform(df)\n",
    "\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def bootstrap_metrics(test_df, model, num_samples=5):\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='tx_fraud', predictionCol=\"prediction\")\n",
    "    metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        sample_df = test_df.sample(withReplacement=True, fraction=0.1, seed=np.random.randint(0, 10000))\n",
    "        predictions = model.transform(sample_df)\n",
    "        \n",
    "        metrics['accuracy'].append(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\n",
    "        metrics['precision'].append(evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "        metrics['recall'].append(evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"}))\n",
    "        metrics['f1'].append(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_z_test(new_metrics, prev_metrics):\n",
    "    z_scores = {}\n",
    "    p_values = {}\n",
    "    \n",
    "    for metric in new_metrics:\n",
    "        new_mean = np.mean(new_metrics[metric])\n",
    "        new_std = np.std(new_metrics[metric])\n",
    "        prev_mean = prev_metrics[metric][0]\n",
    "        prev_std = prev_metrics[metric][1]\n",
    "        \n",
    "        z_score = (new_mean - prev_mean) / np.sqrt(new_std**2 + prev_std**2)\n",
    "        p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
    "        \n",
    "        z_scores[metric] = z_score\n",
    "        p_values[metric] = p_value\n",
    "    \n",
    "    return z_scores, p_values\n",
    "\n",
    "def log_ab_test_results(new_metrics, prev_metrics, z_scores, p_values, run_id, alpha=0.05):\n",
    "    for metric in new_metrics:\n",
    "        mlflow.log_metric(f\"new_{metric}_mean\", np.mean(new_metrics[metric]))\n",
    "        mlflow.log_metric(f\"new_{metric}_std\", np.std(new_metrics[metric]))\n",
    "        mlflow.log_metric(f\"new_{metric}_2.5th_percentile\", np.percentile(new_metrics[metric], 2.5))\n",
    "        mlflow.log_metric(f\"new_{metric}_97.5th_percentile\", np.percentile(new_metrics[metric], 97.5))\n",
    "        \n",
    "        if prev_metrics:\n",
    "            p_value = p_values[metric]\n",
    "            z_score = z_scores[metric]\n",
    "            mlflow.log_metric(f\"{metric}_z_score\", z_score)\n",
    "            mlflow.log_metric(f\"{metric}_p_value\", p_value)\n",
    "            \n",
    "            if p_value < alpha:\n",
    "                mlflow.log_text(\n",
    "                    f\"{metric} has statistically significant difference (p_value={p_value}, alpha={alpha})\",\n",
    "                    f\"a_b_test_results_for_{metric}.txt\")\n",
    "            else:\n",
    "                mlflow.log_text(\n",
    "                    f\"{metric} does not have statistically significant difference (p_value={p_value}, alpha={alpha})\",\n",
    "                    f\"a_b_test_results_for_{metric}.txt\")\n",
    "\n",
    "def ab_test(test_df, model, run_id, client, experiment_id):\n",
    "    new_metrics = bootstrap_metrics(test_df, model)\n",
    "    \n",
    "    prev_run = client.search_runs(experiment_ids=[experiment_id], order_by=[\"start_time DESC\"], max_results=2)\n",
    "    if len(prev_run) > 1:\n",
    "        prev_run_id = prev_run[1].info.run_id\n",
    "        prev_metrics = {}\n",
    "        for metric in new_metrics:\n",
    "            prev_metrics[metric]  = (client.get_metric_history(prev_run_id, f\"new_{metric}_mean\")[0].value,\n",
    "                                     client.get_metric_history(prev_run_id, f\"new_{metric}_std\")[0].value)\n",
    "            \n",
    "        z_scores, p_values = calculate_z_test(new_metrics, prev_metrics)\n",
    "        log_ab_test_results(new_metrics, prev_metrics, z_scores, p_values, run_id)\n",
    "    else:\n",
    "        prev_metrics = {}\n",
    "        z_scores = {}\n",
    "        p_values = {}\n",
    "        log_ab_test_results(new_metrics, prev_metrics, z_scores, p_values, run_id)\n",
    "        logger.info(\"No previous run found. Just logging metrics stats.\")\n",
    "\n",
    "def main(args):\n",
    "    TRACKING_SERVER_HOST = \"158.160.38.148\"\n",
    "    mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:8000\")\n",
    "    logger.info(\"tracking URI: %s\", {mlflow.get_tracking_uri()})\n",
    "\n",
    "    logger.info(\"Creating Spark Session ...\")\n",
    "\n",
    "    conf = (\n",
    "        SparkConf().setMaster(\"yarn\").setAppName(\"Fraud Detection\")\n",
    "            .set(\"spark.executor.memory\", \"2g\")\n",
    "            .set(\"spark.driver.memory\", \"4g\")\n",
    "            .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "            .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            .set(\"spark.hadoop.fs.s3a.access.key\", \"\")\n",
    "            .set(\"spark.hadoop.fs.s3a.secret.key\", \"\")\n",
    "            .set(\"spark.hadoop.fs.s3a.endpoint\", \"storage.yandexcloud.net\")\n",
    "    )\n",
    "\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "    logger.info(\"Loading Data ...\")\n",
    "    df = get_dataframe(spark)\n",
    "\n",
    "    # Prepare MLFlow experiment for logging\n",
    "    client = MlflowClient()\n",
    "    experiment = client.get_experiment_by_name(\"ab_pyspark_experiment\")\n",
    "    experiment_id = experiment.experiment_id\n",
    "    \n",
    "    # Add a unique name to your run\n",
    "    run_name = 'MyRFmodelRUN' + ' ' + str(datetime.now())\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name, experiment_id=experiment_id):\n",
    "\n",
    "        df = preproc(df)\n",
    "        df = scale(df)\n",
    "\n",
    "        logger.info(\"Splitting the dataset ...\")\n",
    "        train_df, test_df = df.randomSplit([1 - args.val_frac, args.val_frac], seed=42)\n",
    "\n",
    "        rf_classifier = RandomForestClassifier(featuresCol='scaled_features', labelCol='tx_fraud', numTrees=100, seed=42)\n",
    "        model = rf_classifier.fit(train_df)\n",
    "\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "        logger.info(\"Performing AB test ...\")\n",
    "        ab_test(test_df, model, run_id, client, experiment_id)\n",
    "\n",
    "        logger.info(\"Saving pipeline ...\")\n",
    "        mlflow.spark.save_model(model, args.output_artifact)\n",
    "\n",
    "        # logger.info(\"Exporting/logging pipeline ...\")\n",
    "        # mlflow.spark.log_model(model, args.output_artifact, dfs_tmpdir='/home/ubuntu/tmp/mlflow')\n",
    "\n",
    "\n",
    "        logger.info(\"Done\")\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c14762a3-3898-4cac-a509-0cebd2811aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         description=\"Model (Inference Pipeline) Training\")\n",
    "\n",
    "#     parser.add_argument(\n",
    "#         \"--val_frac\",\n",
    "#         type=float,\n",
    "#         default = 0.2,\n",
    "#         help=\"Size of the validation split. Fraction of the dataset.\",\n",
    "#     )\n",
    "\n",
    "#     # При запуске используйте оригинальное имя 'Student_Name_flights_LR_only'\n",
    "#     parser.add_argument(\n",
    "#         \"--output_artifact\",\n",
    "#         default=\"default_run_name\",\n",
    "#         type=str,\n",
    "#         help=\"Name for the output serialized model (Inference Artifact folder)\",\n",
    "#         required=True,\n",
    "#     )\n",
    "    \n",
    "#     sys.argv = ['train.ipynb', '--val_frac', '0.2', '--output_artifact', 'run-name']\n",
    "#     args = parser.parse_args(sys.argv[1:])\n",
    "\n",
    "#     # args = parser.parse_args()\n",
    "\n",
    "#     main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719196cb-06b7-4e32-a4eb-415c0a56e3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 20:05:42,896 tracking URI: {'http://158.160.38.148:8000'}\n",
      "2024-07-15 20:05:42,897 Creating Spark Session ...\n",
      "2024-07-15 20:05:51,388 Loading Data ...\n",
      "2024-07-15 20:06:39,646 Splitting the dataset ...\n",
      "2024-07-15 20:08:41,379 Performing AB test ...\n"
     ]
    }
   ],
   "source": [
    "sys.argv = ['train.ipynb', '--val_frac', '0.2', '--output_artifact', 'run-name']\n",
    "args = parser.parse_args(sys.argv[1:])\n",
    "    \n",
    "TRACKING_SERVER_HOST = \"158.160.38.148\"\n",
    "mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:8000\")\n",
    "logger.info(\"tracking URI: %s\", {mlflow.get_tracking_uri()})\n",
    "\n",
    "logger.info(\"Creating Spark Session ...\")\n",
    "\n",
    "conf = (\n",
    "    SparkConf().setMaster(\"yarn\").setAppName(\"Fraud Detection\")\n",
    "        .set(\"spark.executor.memory\", \"2g\")\n",
    "        .set(\"spark.driver.memory\", \"4g\")\n",
    "        .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", \"\")\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", \"\")\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", \"storage.yandexcloud.net\")\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "logger.info(\"Loading Data ...\")\n",
    "df = get_dataframe(spark)\n",
    "\n",
    "# Prepare MLFlow experiment for logging\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(\"ab_pyspark_experiment\")\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Add a unique name to your run\n",
    "run_name = 'MyRFmodelRUN' + ' ' + str(datetime.now())\n",
    "\n",
    "with mlflow.start_run(run_name=run_name, experiment_id=experiment_id):\n",
    "\n",
    "    df = preproc(df)\n",
    "    df = scale(df)\n",
    "\n",
    "    logger.info(\"Splitting the dataset ...\")\n",
    "    train_df, test_df = df.randomSplit([1 - args.val_frac, args.val_frac], seed=42)\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(featuresCol='scaled_features', labelCol='tx_fraud', numTrees=100, seed=42)\n",
    "    model = rf_classifier.fit(train_df)\n",
    "\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "    logger.info(\"Performing AB test ...\")\n",
    "    ab_test(test_df, model, run_id, client, experiment_id)\n",
    "\n",
    "    logger.info(\"Saving pipeline ...\")\n",
    "    mlflow.spark.save_model(model, args.output_artifact)\n",
    "\n",
    "    # logger.info(\"Exporting/logging pipeline ...\")\n",
    "    # mlflow.spark.log_model(model, args.output_artifact, dfs_tmpdir='/home/ubuntu/tmp/mlflow')\n",
    "\n",
    "    logger.info(\"Done\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "44e8e60d-ccc1-4cbd-bd47-f3edb14fdae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare MLFlow experiment for logging\n",
    "# client = MlflowClient()\n",
    "# experiment = client.get_experiment_by_name(\"ab_pyspark_experiment\")\n",
    "# experiment_id = experiment.experiment_id\n",
    "\n",
    "# new_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "# prev_run = client.search_runs(experiment_ids=[experiment_id], order_by=[\"start_time DESC\"], max_results=2)\n",
    "# if len(prev_run) >= 1:\n",
    "#     prev_run_id = prev_run[1].info.run_id\n",
    "#     prev_metrics = {}\n",
    "#     for metric in new_metrics:\n",
    "#         prev_metrics[metric]  = (client.get_metric_history(prev_run_id, f\"new_{metric}_mean\")[0].value,\n",
    "#                                      client.get_metric_history(prev_run_id, f\"new_{metric}_std\")[0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f47e9d40-7b05-4986-9a3d-4d50beaf2eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9264647400963135"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_metric_history(prev_run_id, f\"new_{metric}_mean\")[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "867b6305-15af-421c-a3cf-28a0eb282f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f71c8b-876a-46ea-8ece-25702e505a08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
