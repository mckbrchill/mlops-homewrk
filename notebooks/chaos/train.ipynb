{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b688bd1a-86b4-45b3-814e-e74f3a6fcddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark==2.0.1 in ./.local/lib/python3.8/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark==2.0.1\n",
    "# !pip install pandas==2.0.0\n",
    "# !pip install numpy==1.24.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9adca449-f61f-493b-85d5-d0610a77c2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/spark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3916643-8d48-41d4-913c-ba4c2d7c349b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-2.14.1-py3-none-any.whl (25.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.8 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz<2025 in /opt/conda/lib/python3.8/site-packages (from mlflow) (2023.3.post1)\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.8/site-packages (from mlflow) (0.23.2)\n",
      "Requirement already satisfied: packaging<25 in /opt/conda/lib/python3.8/site-packages (from mlflow) (23.2)\n",
      "Collecting graphene<4\n",
      "  Downloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 86.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opentelemetry-sdk<3,>=1.9.0\n",
      "  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 91.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting querystring-parser<2\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting Flask<4\n",
      "  Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.8/site-packages (from mlflow) (0.4)\n",
      "Collecting cloudpickle<4\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11; platform_system != \"Windows\" in /opt/conda/lib/python3.8/site-packages (from mlflow) (3.1.3)\n",
      "Requirement already satisfied: pandas<3 in ./.local/lib/python3.8/site-packages (from mlflow) (2.0.0)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Downloading sqlparse-0.5.0-py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 2.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.8/site-packages (from mlflow) (6.0.1)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.8/site-packages (from mlflow) (7.0.1)\n",
      "Requirement already satisfied: numpy<2 in ./.local/lib/python3.8/site-packages (from mlflow) (1.24.2)\n",
      "Collecting click<9,>=7.0\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 4.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6,>=5.0.0\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting gunicorn<23; platform_system != \"Windows\"\n",
      "  Downloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.8/site-packages (from mlflow) (1.9.3)\n",
      "Collecting opentelemetry-api<3,>=1.9.0\n",
      "  Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 3.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting markdown<4,>=3.3\n",
      "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 89.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow<16,>=4.0.0\n",
      "  Downloading pyarrow-15.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.5 MB 113 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from mlflow) (3.17.1)\n",
      "Collecting docker<8,>=4.0.0\n",
      "  Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 86.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.8/site-packages (from mlflow) (2.24.0)\n",
      "Collecting alembic!=1.10.0,<2\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "\u001b[K     |████████████████████████████████| 233 kB 87.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from mlflow) (2.0.25)\n",
      "Collecting gitpython<4,>=3.1.9\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[K     |████████████████████████████████| 207 kB 90.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.8/site-packages (from mlflow) (3.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<2->mlflow) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<2->mlflow) (1.2.0)\n",
      "Collecting graphql-core<3.3,>=3.1\n",
      "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
      "\u001b[K     |████████████████████████████████| 202 kB 87.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting graphql-relay<3.3,>=3.1\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting aniso8601<10,>=8\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 2.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting opentelemetry-semantic-conventions==0.46b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 86.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.8/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow) (4.9.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from querystring-parser<2->mlflow) (1.16.0)\n",
      "Collecting Werkzeug>=3.0.0\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[K     |████████████████████████████████| 227 kB 83.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting itsdangerous>=2.1.2\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting blinker>=1.6.2\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from Jinja2<4,>=2.11; platform_system != \"Windows\"->mlflow) (2.1.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.local/lib/python3.8/site-packages (from pandas<3->mlflow) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from pandas<3->mlflow) (2.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.17.0)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting urllib3>=1.26.0\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 81.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (2024.2.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (2.10)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 6.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /opt/conda/lib/python3.8/site-packages (from alembic!=1.10.0,<2->mlflow) (6.1.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))) in /opt/conda/lib/python3.8/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.1)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 634 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow) (1.4.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib<4->mlflow) (3.0.9)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 765 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: graphql-core, graphql-relay, aniso8601, graphene, wrapt, deprecated, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-sdk, querystring-parser, Werkzeug, click, itsdangerous, blinker, Flask, cloudpickle, sqlparse, cachetools, gunicorn, markdown, pyarrow, urllib3, docker, Mako, alembic, smmap, gitdb, gitpython, mlflow\n",
      "\u001b[33m  WARNING: The script flask is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script sqlformat is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script gunicorn is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script markdown_py is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script mako-render is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script alembic is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script mlflow is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "requests 2.24.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 2.2.2 which is incompatible.\n",
      "koalas 1.7.0 requires numpy<1.20.0,>=1.14, but you'll have numpy 1.24.2 which is incompatible.\n",
      "koalas 1.7.0 requires pandas<1.2.0,>=0.23.2, but you'll have pandas 2.0.0 which is incompatible.\n",
      "botocore 1.19.7 requires urllib3<1.26,>=1.25.4, but you'll have urllib3 2.2.2 which is incompatible.\n",
      "docker 7.1.0 requires requests>=2.26.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n",
      "Successfully installed Flask-3.0.3 Mako-1.3.5 Werkzeug-3.0.3 alembic-1.13.1 aniso8601-9.0.1 blinker-1.8.2 cachetools-5.3.3 click-8.1.7 cloudpickle-3.0.0 deprecated-1.2.14 docker-7.1.0 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-22.0.0 itsdangerous-2.2.0 markdown-3.6 mlflow-2.14.1 opentelemetry-api-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 pyarrow-15.0.2 querystring-parser-1.2.4 smmap-5.0.1 sqlparse-0.5.0 urllib3-2.2.2 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "187124eb-c433-4f9b-a08b-66dc2dc280ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'predictionCol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8aa85e294bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetricName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'predictionCol'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Fraud Detection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# S3 path\n",
    "s3_path = 's3a://otus-task-n3/2019-09-21.txt'\n",
    "\n",
    "# Read data from S3 into Spark DataFrame\n",
    "df = spark.read.parquet(s3_path, \n",
    "                        header=True, \n",
    "                        inferSchema=True,\n",
    "                        **{\n",
    "                            \"key\": \"YCAJE4JxJM9HxqbDEUlXzJJsX\",\n",
    "                            \"secret\": \"YCNH45SiJbpy_35ywf2KolGlHKfB5lLZtbgmw2xk\",\n",
    "                            \"client_kwargs\": {\"endpoint_url\": \"https://storage.yandexcloud.net\"}\n",
    "                        })\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df = df.withColumn('tx_datetime', F.to_timestamp(df['tx_datetime']))\n",
    "\n",
    "# Time-based features\n",
    "df = df.withColumn('is_weekend', F.when(F.dayofweek(df['tx_datetime']) >= 5, 1).otherwise(0))\n",
    "\n",
    "# Sort by customer and transaction datetime\n",
    "df = df.orderBy(['customer_id', 'tx_datetime'])\n",
    "\n",
    "# Customer behavior features\n",
    "window_spec = Window.partitionBy('customer_id').orderBy('tx_datetime')\n",
    "# Add a lag column as a timestamp\n",
    "df = df.withColumn('lagged_tx_datetime', F.lag('tx_datetime', 1).over(window_spec).cast('timestamp'))\n",
    "\n",
    "# Calculate time_since_last_tx in seconds\n",
    "df = df.withColumn('time_since_last_tx', \n",
    "                   (F.col('tx_datetime').cast('long') - F.col('lagged_tx_datetime').cast('long')) / F.lit(1000))\n",
    "\n",
    "# Drop the intermediate lagged_tx_datetime column if not needed\n",
    "df = df.drop('lagged_tx_datetime')\n",
    "\n",
    "df = df.withColumn('avg_tx_amount_customer', F.avg('tx_amount').over(window_spec))\n",
    "df = df.withColumn('tx_count_customer', F.count('tranaction_id').over(window_spec))\n",
    "df = df.withColumn('var_tx_amount_customer', F.stddev('tx_amount').over(window_spec))\n",
    "\n",
    "# Terminal-based features\n",
    "window_spec_terminal = Window.partitionBy('terminal_id').orderBy('tx_datetime')\n",
    "df = df.withColumn('avg_tx_amount_terminal', F.avg('tx_amount').over(window_spec_terminal))\n",
    "df = df.withColumn('tx_count_terminal', F.count('tranaction_id').over(window_spec_terminal))\n",
    "df = df.withColumn('var_tx_amount_terminal', F.stddev('tx_amount').over(window_spec_terminal))\n",
    "\n",
    "# Select relevant numeric columns\n",
    "numeric_columns = ['tx_amount', 'time_since_last_tx', 'avg_tx_amount_customer', 'tx_count_customer',\n",
    "                   'var_tx_amount_customer', 'avg_tx_amount_terminal', 'tx_count_terminal', 'var_tx_amount_terminal']\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"scaled_features\", outputCol=\"features\")\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Fit and transform the numeric columns\n",
    "assembler = VectorAssembler(inputCols=numeric_columns, outputCol=\"scaled_features\")\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "pipeline_model = pipeline.fit(df)\n",
    "df = pipeline_model.transform(df)\n",
    "\n",
    "# Convert boolean column to binary (1/0)\n",
    "df = df.withColumn('is_weekend', F.col('is_weekend').cast('integer'))\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Define feature columns and target variable\n",
    "feature_columns = ['scaled_features', 'is_weekend']\n",
    "target_column = 'tx_fraud'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(featuresCol='scaled_features', labelCol=target_column, numTrees=100, seed=42)\n",
    "\n",
    "# Train the model\n",
    "model = rf_classifier.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Optionally, you can print confusion matrix and classification report using other evaluation metrics supported by PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49d32138-0164-4feb-acd0-728abb698b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9495932235247968\n",
      "Precision: 0.944067005065678\n",
      "Recall: 0.9495932235247969\n",
      "F1-score: 0.92602191718943\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Evaluate the model with MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=target_column,predictionCol=\"prediction\")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c1cc971-bb7f-4310-ad90-b0ec3f60fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72655ef7-be94-4e49-aafd-1c3daf876308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d1f687-3f7f-4977-96a0-836582d1cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62226153-1725-4859-940a-eb446e8ff8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 00:18:05,982 tracking URI: {'http://62.84.126.15:8000'}\n",
      "2024-06-25 00:18:05,983 Creating Spark Session ...\n",
      "2024-06-25 00:18:05,986 Loading Data ...\n",
      "2024-06-25 00:18:42,561 Splitting the dataset ...\n",
      "2024-06-25 00:21:26,115 Scoring the model ...\n",
      "2024-06-25 00:23:59,462 Logging metrics to MLflow run acfd3c9cd8e74717af9c901c03c002ee ...\n",
      "2024-06-25 00:23:59,555 Model accuracy: 0.9497801349811982\n",
      "2024-06-25 00:23:59,556 Model accuracy: 0.9497801349811982\n",
      "2024-06-25 00:23:59,557 Model accuracy: 0.9466131236796039\n",
      "2024-06-25 00:23:59,559 Model accuracy: 0.9261529216620966\n",
      "2024-06-25 00:23:59,560 Saving pipeline ...\n",
      "2024/06/25 00:24:20 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - pandas (current: 2.0.0, required: pandas<2)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2024-06-25 00:24:20,244 Exporting/logging pipline ...\n",
      "2024/06/25 00:24:38 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - pandas (current: 2.0.0, required: pandas<2)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2024-06-25 00:24:38,624 Found credentials in environment variables.\n",
      "2024-06-25 00:24:39,287 Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)-15s %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"https://storage.yandexcloud.net\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"\"\n",
    "\n",
    "\n",
    "def get_dataframe(spark):\n",
    "    \n",
    "    s3_path = 's3a://otus-task-n3/2019-09-21.txt'\n",
    "    df = spark.read.parquet(s3_path, \n",
    "                            header=True, \n",
    "                            inferSchema=True,\n",
    "                            **{\n",
    "                                \"key\": \"\",\n",
    "                                \"secret\": \"\",\n",
    "                                \"client_kwargs\": {\"endpoint_url\": \"https://storage.yandexcloud.net\"}\n",
    "                            })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preproc(df):\n",
    "    \n",
    "    # Assuming df is DataFrame\n",
    "    df = df.withColumn('tx_datetime', F.to_timestamp(df['tx_datetime']))\n",
    "    # Time-based features\n",
    "    df = df.withColumn('is_weekend', F.when(F.dayofweek(df['tx_datetime']) >= 5, 1).otherwise(0))\n",
    "    # Sort by customer and transaction datetime\n",
    "    df = df.orderBy(['customer_id', 'tx_datetime'])\n",
    "    # Customer behavior features\n",
    "    window_spec = Window.partitionBy('customer_id').orderBy('tx_datetime')\n",
    "    # Add a lag column as a timestamp\n",
    "    df = df.withColumn('lagged_tx_datetime', F.lag('tx_datetime', 1).over(window_spec).cast('timestamp'))\n",
    "    # Calculate time_since_last_tx in seconds\n",
    "    df = df.withColumn('time_since_last_tx', \n",
    "                       (F.col('tx_datetime').cast('long') - F.col('lagged_tx_datetime').cast('long')) / F.lit(1000))\n",
    "    # Drop the intermediate lagged_tx_datetime column if not needed\n",
    "    df = df.drop('lagged_tx_datetime')\n",
    "    df = df.withColumn('avg_tx_amount_customer', F.avg('tx_amount').over(window_spec))\n",
    "    df = df.withColumn('tx_count_customer', F.count('tranaction_id').over(window_spec))\n",
    "    df = df.withColumn('var_tx_amount_customer', F.stddev('tx_amount').over(window_spec))\n",
    "    # Terminal-based features\n",
    "    window_spec_terminal = Window.partitionBy('terminal_id').orderBy('tx_datetime')\n",
    "    df = df.withColumn('avg_tx_amount_terminal', F.avg('tx_amount').over(window_spec_terminal))\n",
    "    df = df.withColumn('tx_count_terminal', F.count('tranaction_id').over(window_spec_terminal))\n",
    "    df = df.withColumn('var_tx_amount_terminal', F.stddev('tx_amount').over(window_spec_terminal))\n",
    "    \n",
    "    # Convert boolean column to binary (1/0)\n",
    "    df = df.withColumn('is_weekend', F.col('is_weekend').cast('integer'))\n",
    "    # Drop rows with null values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def scale(df):\n",
    "    \n",
    "    numeric_columns = ['tx_amount', 'time_since_last_tx', 'avg_tx_amount_customer', 'tx_count_customer',\n",
    "                       'var_tx_amount_customer', 'avg_tx_amount_terminal', 'tx_count_terminal', 'var_tx_amount_terminal']\n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"scaled_features\", outputCol=\"features\")\n",
    "    assembler = VectorAssembler(inputCols=numeric_columns, outputCol=\"scaled_features\")\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    df = pipeline_model.transform(df)\n",
    "\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    TRACKING_SERVER_HOST = \"62.84.126.15\"\n",
    "    mlflow.set_tracking_uri(f\"http://{TRACKING_SERVER_HOST}:8000\")\n",
    "    logger.info(\"tracking URI: %s\", {mlflow.get_tracking_uri()})\n",
    "\n",
    "    logger.info(\"Creating Spark Session ...\")\n",
    "    spark = SparkSession.builder \\\n",
    "            .appName(\"Fraud Detection\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", \"\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", \"\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", \"storage.yandexcloud.net\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    logger.info(\"Loading Data ...\")\n",
    "    df = get_dataframe(spark)\n",
    "\n",
    "    # Prepare MLFlow experiment for logging\n",
    "    client = MlflowClient()\n",
    "    experiment = client.get_experiment_by_name(\"pyspark_experiment\")\n",
    "    experiment_id = experiment.experiment_id\n",
    "    \n",
    "    # Добавьте в название вашего run имя, по которому его можно будет найти в MLFlow\n",
    "    run_name = 'MyRFmodelRUN' + ' ' + str(datetime.now())\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name, experiment_id=experiment_id):\n",
    "    \n",
    "        df = preproc(df)\n",
    "        df = scale(df)\n",
    "        \n",
    "        logger.info(\"Splitting the dataset ...\")\n",
    "        train_df, test_df = df.randomSplit([1 - args.val_frac, args.val_frac], seed=42)\n",
    "\n",
    "        rf_classifier = RandomForestClassifier(featuresCol='scaled_features', labelCol='tx_fraud', numTrees=100, seed=42)\n",
    "        model = rf_classifier.fit(train_df)\n",
    "        \n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "        logger.info(\"Scoring the model ...\")\n",
    "        predictions = model.transform(test_df)\n",
    "        \n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol='tx_fraud',predictionCol=\"prediction\")\n",
    "\n",
    "        accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "        precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "        recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "        f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "        \n",
    "        logger.info(f\"Logging metrics to MLflow run {run_id} ...\")\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"f1\", f1)\n",
    "        logger.info(f\"Model accuracy: {accuracy}\")\n",
    "        logger.info(f\"Model accuracy: {recall}\")\n",
    "        logger.info(f\"Model accuracy: {precision}\")\n",
    "        logger.info(f\"Model accuracy: {f1}\")\n",
    "\n",
    "        logger.info(\"Saving pipeline ...\")\n",
    "        mlflow.spark.save_model(model, args.output_artifact)\n",
    "\n",
    "        logger.info(\"Exporting/logging pipline ...\")\n",
    "        mlflow.spark.log_model(model, args.output_artifact)\n",
    "        logger.info(\"Done\")\n",
    "\n",
    "    spark.stop()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Model (Inference Pipeline) Training\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--val_frac\",\n",
    "        type=float,\n",
    "        default = 0.2,\n",
    "        help=\"Size of the validation split. Fraction of the dataset.\",\n",
    "    )\n",
    "\n",
    "    # При запуске используйте оригинальное имя 'Student_Name_flights_LR_only'\n",
    "    parser.add_argument(\n",
    "        \"--output_artifact\",\n",
    "        default=\"default_run_name\",\n",
    "        type=str,\n",
    "        help=\"Name for the output serialized model (Inference Artifact folder)\",\n",
    "        required=True,\n",
    "    )\n",
    "    \n",
    "    sys.argv = ['train.ipynb', '--val_frac', '0.2', '--output_artifact', 'run-name']\n",
    "    args = parser.parse_args(sys.argv[1:])\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4adb2405-4164-4617-8b6a-3a5211d60928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting urllib3<2\n",
      "  Downloading urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[K     |████████████████████████████████| 143 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.2\n",
      "    Uninstalling urllib3-2.2.2:\n",
      "      Successfully uninstalled urllib3-2.2.2\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "docker 7.1.0 requires requests>=2.26.0, but you'll have requests 2.24.0 which is incompatible.\n",
      "requests 2.24.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.19 which is incompatible.\n",
      "botocore 1.19.7 requires urllib3<1.26,>=1.25.4, but you'll have urllib3 1.26.19 which is incompatible.\u001b[0m\n",
      "Successfully installed urllib3-1.26.19\n"
     ]
    }
   ],
   "source": [
    "!pip install 'urllib3<2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acf50d01-63fb-4879-90ee-c466e52c06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff92e37-dc78-47e8-8f14-a212bb135557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
